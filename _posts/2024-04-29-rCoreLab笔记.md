---
title: rCoreLab笔记
Author: SaZiKK
categories:
  - OS
  - study
date create: 2024-04-29 14:24:46 +0800
date: 2024-05-06 23:10:08 +0800
tags:
  - os
  - rCore
---
完成rCoreLab的学习笔记。实验指导手册：[rCore-Tutorial-Guide-2024S 文档 (learningos.cn)](https://learningos.cn/rCore-Tutorial-Guide-2024S/)

ch1和2没有lab，所以实验从ch3开始。在实验开始前，我已经完整阅读了[rCore-Tutorial-Book 第三版](https://rcore-os.cn/rCore-Tutorial-Book-v3/chapter0/index.html)直到ch6的内容以及代码，因此在代码理解上没有很大的障碍。
## Chapter3    实现`sys_task_info`系统调用
### 代码部分：
该系统调用要求获取任务控制块相关信息（任务状态）、任务使用的系统调用及调用次数、系统调用时刻距离任务第一次被调度时刻的时长（单位ms）。
```rust
struct TaskInfo {
    status: TaskStatus,
    syscall_times: [u32; MAX_SYSCALL_NUM],
    time: usize
}
```
由于获取的是当前任务，因此状态一定为`Running`，下面着重分析统计系统调用次数和计时的实现。

首先是统计并获取系统调用次数，我采取的方法是在TCB中维护系统调用次数，然后为`TaskManager`实现对应方法维护并获取。
```rust
//os/src/task/task.rs

pub struct TaskControlBlock {
    /// The task status in it's lifecycle
    pub task_status: TaskStatus,
    /// The task context
    pub task_cx: TaskContext,
    /// the syscall times
    pub syscall_times: [u32; MAX_SYSCALL_NUM],
}
```

```rust
//os/src/task/mod.rs impl TaskManager

///update syscall times and get syscall times
fn count_syscall_times(&self, id: usize) -> [u32; MAX_SYSCALL_NUM] {
	let mut inner = self.inner.exclusive_access();
	let current = inner.current_task;
	if id <= MAX_SYSCALL_NUM {
		inner.tasks[current].syscall_times[id] += 1;
	}
	inner.tasks[current].syscall_times
}
```
最后在`Traphandler`中系统调用的入口处调用该方法实现系统调用计数，并在syscall中调用方法查询计数。在syscall函数里的方法调用传入了一个越界的数，这样就不会修改系统调用的计数，仅返回查询到的结果。
```rust
//os/src/trap/mod.rs

pub fn trap_handler(cx: &mut TrapContext) -> &mut TrapContext {
    let scause = scause::read(); // get trap cause
    let stval = stval::read(); // get extra value
                               // trace!("into {:?}", scause.cause());
    match scause.cause() {
        Trap::Exception(Exception::UserEnvCall) => {
            let syscall_id = cx.x[17];
             //count syscall times
             count_syscall_times_from_current(syscall_id);
            // jump to next instruction anyway
            cx.sepc += 4;
            // get system call return value
            cx.x[10] = syscall(cx.x[17], [cx.x[10], cx.x[11], cx.x[12]]) as usize;
        }
        ...
}
```
值得注意的是，上述实现方法并不好，TCB作为一个经常需要被复制和移动的资源应该尽可能简洁，只存放进程本身需要的数据，这样的写法会影响性能且耦合度高，更优的写法是在`TaskManager`中统一管理各个进程的syscall次数。当然更优只针对ch3的代码，在后续我们对进程的操作和管理趋于复杂时，`TaskManager`里存放的数据也应该和对应进程的生命周期同步销毁，需要额外花精力去维护。

在实现获取进程运行时间时，我一开始误解了题意，排除了并发的影响，维护了进程实际运行时间。我借鉴了练习题中统计应用完成时间的设计，在TCB中维护了进程在内核态和用户态的执行时间，这样虽然避免了并发带来的影响，但是题意要求的是计算**包含并发调度开销在内**的总时长。下图是打印出的调试信息，可以看到实际时间和计算并发后的时间相差了接近一倍。
![](../assets/figures/Pasted%20image%2020240430162904.png)
修改实现思路后，我在TaskManager里为所有进程维护了第一次调度时间`task_start_time`，并在`run_first_task`  `run_next_task`方法中更新，这样只要在查询时间时，用`get_time_ms`获取当前时间作差即可
```rust
//os/src/task/mod.rs

pub struct TaskManagerInner {
    /// task list
    tasks: [TaskControlBlock; MAX_APP_NUM],
    /// id of current `Running` task
    current_task: usize,
    last_stop_time: usize,
    task_start_time: [usize; MAX_APP_NUM],
}

impl TaskManagerInner{
    fn run_first_task(&self) -> ! {
        ...
        //开始计时
        inner.task_start_time[0] = get_time_ms();
        inner.refresh_last_stop_time();
        ...
    }

    fn run_next_task(&self) {
        if let Some(next) = self.find_next_task() {
            ...
            //如果是第一次调用，维护第一次调度时间
            if inner.task_start_time[next] == 0 {
                inner.task_start_time[next] = get_time_ms();
            }
            ...
        } else {
            panic!("All applications completed!");
        }
    }
    ///在内核态获取运行时间
    fn count_task_time(&self) -> usize {
        let inner = self.inner.exclusive_access();
        let current = inner.current_task;
        let total_time = get_time_ms() - inner.task_start_time[current];
        total_time
    }
}

///get task process time
pub fn count_current_task_time() -> usize {
    TASK_MANAGER.count_task_time()
} 
```
最后将各个接口打包成syscall函数，chapter3的lab代码部分就完成了
```rust
pub fn sys_task_info(_ti: *mut TaskInfo) -> isize {
    trace!("kernel: sys_task_info");
    //id设置为超过上限的数表示查询
    let syscall_times = count_syscall_times_from_current(MAX_SYSCALL_NUM + 1);
    let time = count_current_task_time();
    unsafe{
       (*_ti) = TaskInfo{
        status: TaskStatus::Running,
        syscall_times,
        time,
        }; 
    } 
    0
}
```
### 简答部分：
#### 深入理解 [trap.S](https://github.com/LearningOS/rCore-Tutorial-Code-2024S/blob/ch3/os/src/trap/trap.S) 中两个函数 `__alltraps` 和 `__restore` 的作用，并回答如下问题:
#### 1. L40：刚进入 `__restore` 时，`a0` 代表了什么值。请指出 `__restore` 的两种使用情景。
`__restore`是trap后返回用户态的处理函数，因此此时a0的值应该是系统调用函数返回值或异常处理函数返回值。`__restore`在系统调用返回或异常处理返回时被调用。
#### 2. L43-L48：这几行汇编代码特殊处理了哪些寄存器？这些寄存器的的值对于进入用户态有何意义？请分别解释。
```Assembly
ld t0, 32*8(sp)
ld t1, 33*8(sp)
ld t2, 2*8(sp)
csrw sstatus, t0
csrw sepc, t1
csrw sscratch, t2
```
上述代码使用了三个CSR特权寄存器`sstatus` `sepc`和`sscratch`，在进入trap时，`sstatus`和`sepc`的值会被瞬间覆盖掉。其中`sstatus`用来存储和控制CPU的各种特权和状态信息，这里是恢复trap处理前存储的信息；`sepc`寄存器用来存储中断处下一条指令的位置，恢复`sepc`的值以从中断处继续执行；`sscratch`寄存器的值不会因为特权级切换等原因改变，用来保存一些临时数据，如中断上下文等，这里恢复的是用户栈位置；以上三个特权级寄存器只有在S mode才能使用。
#### 3. L50-L56：为何跳过了 `x2` 和 `x4`？

```Assembly
ld x1, 1*8(sp)
ld x3, 3*8(sp)
.set n, 5
.rept 27
   LOAD_GP %n
   .set n, n+1
.endr
```
tp(x4)一般不会被用到，除非哦我们手动使用它，因此没有必要保存；而sp(x2)后面还要使用，我们需要依靠栈指针加偏移量来找到其他寄存器应该保存的正确位置。没有保存自然也没必要恢复。
#### 4. L60：该指令之后，`sp` 和 `sscratch` 中的值分别有什么意义？
```Assembly
csrrw sp, sscratch, sp
```
这里是相当于交换了`sscratch`和`sp`的值，执行前`sp`指向内核栈指针，`sscratch`指向用户栈指针；这条指令执行后，`sp`就指向了用户栈，为回到用户态做准备。
#### 5. `__restore`：中发生状态切换在哪一条指令？为何该指令执行之后会进入用户态？
特权级切换发生在`sret`，这条指令用于从一个特权态的trap中返回。`sret`会把特权级恢复到`sstatus`寄存器的SPP字段存储的值，即上一个特权级，这样CPU就回到了用户态。`sret`还会把`sstatus`的SIE字段设置为SPIE( Supervisor Previous Interrupt Enable)字段，即恢复上一个中断使能位的状态，同时SPIE会被设置为1，SPP会被设置为支持的最低特权级（一般是User）。
#### 6. L13：该指令之后，`sp` 和 `sscratch` 中的值分别有什么意义？
```Assembly
csrrw sp, sscratch, sp
```
同4，只不过切换结果相反
#### 7. 从 U 态进入 S 态是哪一条指令发生的？
```Assembly
call trap_handler
```
 如果在异常处理中进行了系统调用，就会切换到S态，因为syscall函数实现中使用了`ecall`指令

到这里chapter3的内容就结束了
## Chapter4    `mmap` 和 `munmap` 匿名映射及重写 `sys_get_time` 和 `sys_task_info`

这章的难点在于引入内存管理机制后代码理解难度大幅上升，想要完成lab需要对rCore采取的内存管理策略以及SV39分页机制有较好的理解。

### 代码部分：
#### 重写 `sys_get_time` 和 `sys_task_info`：
在引入内存空间之后，原先的代码在响应这两个系统调用的时候会触发缺页异常，或者导致死循环。造成这一原因的是，外部的裸指针在传入系统调用时，指向的**仍是用户地址空间的虚拟地址**，因此在内核中原样调用就会产生不可预测的错误（~~有人和内核态响应中断斗智斗勇了一晚上~~）。我们需要调用`translated_byte_buffer`方法对裸指针进行转换：
```rust
    trace!("kernel: sys_get_time");
    let us = get_time_us();
    let token = current_user_token();
    let slices = translated_byte_buffer(token, ts as *const u8, size_of::<TimeVal>());
```
`token`参数可以通`过current_user_token()`获取，这是内核已经实现的接口。

需要注意的是，传入的结构体指针所指向的结构体可能被存在两页中，且因为地址经过翻译，这两段内容的地址并不连续，因此需要把数据以切片的形式分别赋给看可能的两段切片：
```rust
pub fn sys_get_time(ts: *mut TimeVal, _tz: usize) -> isize {
    trace!("kernel: sys_get_time");
    let us = get_time_us();
    let token = current_user_token();
    let slices = translated_byte_buffer(token, ts as *const u8, size_of::<TimeVal>());
    let mut ts = TimeVal {
        sec: us / 1_000_000,
        usec: us % 1_000_000,
    };
    unsafe {
        let time_val_slice = ts.borrow_mut() as *mut TimeVal;
        let time_val_bytes: &[u8] = core::slice::from_raw_parts(
            time_val_slice as *const u8,
            size_of::<TimeVal>()
        );
        let mut offset = 0;
        for slice in slices {
            let slice_len = slice.len();
            slice.copy_from_slice(&time_val_bytes[offset..offset + slice_len]);
            offset += slice_len;
        }
    }
    0
}
```
`sys_taks_info`的实现也是同理，这里分享一种更加简短优雅的写法：
```rust
pub fn sys_task_info(_ti: *mut TaskInfo) -> isize {
    //id设置为超过上限的数表示查询
    let syscall_times = count_syscall_times_current(MAX_SYSCALL_NUM + 1);
    let time = get_current_task_time();
    let mut v = translated_byte_buffer(current_user_token(), _ti as *const u8, size_of::<TaskInfo>());
    let mut ti = TaskInfo{
    status: TaskStatus::Running,
    syscall_times,
    time,
    }; 
    unsafe{
        let mut p = ti.borrow_mut() as *mut TaskInfo as *mut u8;
        for slice in v.iter_mut() {
            let len = slice.len();
            ptr::copy_nonoverlapping(p, slice.as_mut_ptr(), len);
            p = p.add(len);
        }
    } 
    0
}
```
#### 实现`mmap` 和 `munmap` 匿名映射：
在rCore的处理中，内存页帧以逻辑段的形式批量管理，因此申请时也是以逻辑段的单位进行申请。内核中为我们提供了一个包含了申请逻辑段、插入并更新页表的完善接口`insert_framed_area`，我们只需要对其进行包装之后调用就可以实现`mmap`。`insert_framed_area`是为MenorySet实现的函数，而我们在TCB里维护了进程对应的地址空间，因此我们可以通过当前进程来获取当前地址空间并申请内存：
```rust
//os/src/task/mod.rs

impl TaskManager{
        fn alloc_new_space(&self, start_va: VirtAddr,end_va: VirtAddr, permission: MapPermission) -> bool {
        let mut inner = self.inner.exclusive_access();
        let current = inner.current_task;
        if !inner.tasks[current].memory_set.space_check(start_va, end_va) {
            return false;
        }
        inner.tasks[current].memory_set.insert_framed_area(start_va, end_va, permission);
        true
    }
}

/// alloc new space for the task
pub fn insert_new_framed_area(start_va: VirtAddr,end_va: VirtAddr, permission: MapPermission) -> bool {
    TASK_MANAGER.alloc_new_space(start_va, end_va, permission)
}
```
系统调用要求检查申请的空间是否已被占用，因此我们实现了一个`space_check`方法，将当前申请地址与已有的所有逻辑段地址进行比较，确认是否冲突。所有的函数都返回一个bool型变量，以将分配是否成功的信息传递给syscall函数。

最后再维护一下题目指出的可能错误，我们就实现了`mmap`。注意port的格式与MapPermission并不相同，转换前需要调整。
```rust
pub fn sys_mmap(_start: usize, _len: usize, _port: usize) -> isize {
    let start_va = VirtAddr::from(_start);
    let end_va = VirtAddr::from(_start + _len);
    let permission =  MapPermission::from_bits((_port << 1 | 16) as u8).unwrap();
    if _port & !0x7 != 0 || _port & 0x7 == 0 {
       return -1;
    }
    if !start_va.aligned(){
        return -1;
    }
    if !insert_new_framed_area(start_va, end_va, permission) {
        return -1;
    }
    0
}
```

在`munmap`的实现上我偷了点懒，按照UNIX标准，`munmap`应该支持释放整个逻辑段或者从逻辑段头/尾开始释放部分逻辑段，由于测试用例仅仅考察完整释放逻辑段，因此我只实现了针对完整逻辑段的释放和检查。具体的实现方式是在对地址合法性做检查之后，调用MemorySet下的`unmap`方法直接释放，过程较为简单：
```rust
pub fn sys_munmap(_start: usize, _len: usize) -> isize {
    let start_va: VirtAddr = _start.into();
    let end_va: VirtAddr = (_start + _len).into();
    if dealloc_current_space(start_va, end_va) {
        return 0
    }
    -1
}

//os/src/task/mod.rs

impl TaskManager{
    fn dealloc_space(&self, start_va: VirtAddr,end_va: VirtAddr) -> bool {
        let mut inner = self.inner.exclusive_access();
        let current = inner.current_task;
        inner.tasks[current].memory_set.unmap_space(start_va, end_va)
    }
}

///dealloc space
pub fn dealloc_current_space(start_va: VirtAddr,end_va: VirtAddr) -> bool {
    TASK_MANAGER.dealloc_space(start_va, end_va)
}

//os/src/mm/memory_set.rs

/// check if va allign to edge of MapArea, return false when not
impl MamorySet{
    pub fn unmap_space(&mut self, start_va: VirtAddr, end_va: VirtAddr) -> bool {
        if let Some(index) = self.areas.iter().position(|maparea| maparea.is_alligned(start_va, end_va)) {
            self.areas[index].unmap(&mut self.page_table);
            self.areas.remove(index);
            return true;
        }
        false
    }
}
```
以上就是chapter4代码的全部实现，接下来是我个人踩过的一些坑的记录
#### 踩坑记录：
##### 1. 不理解裸指针写入错误原因，试图大力出奇迹
在刚开始上手写`sys_get_time`的时候，我排查出了随机出现的死循环错误或者PageFault来自对于裸指针的写入，但我并没有意识到这实际上是地址空间的不统一造成的，因此我在反复GDB数个小时后得出了需要实现在内核态响应中断的暴论（因为每次PageFault都是靠简陋的`kernel_trap_handler`来panic，并且观察到死循环也是进了__alltraps开始的），于是我又花费了几个小时重构Trap.S以及trap_handler，并且成功在一定程度上解决了问题，我的新版`kernel_trap_handler`真的处理了内核态中断了（~~什么大力出奇迹~~）。直到和队友思考了几个小时PageFault应该如何处理之后，我们才想到，**目前内核态压根不应该有StorePageFault**，**我们连虚拟内存都没有**。发现似乎白通了一宵后，队友痛定思痛，一语点醒梦中人，指出了地址空间不符的问题，我们才得以正确实现写裸指针的操作。

### 简答部分：

暂时鸽了

## Chapter5 实现`sys_spawn`和 stride 调度算法
Chapter5 的主要内容是实现`sys_spawn`系统调用以及stride 调度算法。`sys_spawn`是一个UNIX标准的系统调用，实现难度并不算高，stride的思想也很好理解，这一章的重点在于理解代码框架的修改以及进程管理的实现。
### 代码部分：
#### 支持前面lab的系统调用
开始ch5的第一步就是合并先前在其他分支实现的全部系统调用。使用git cherry-pick系列命令可以大大简化这个过程，但是由于代码框架的大幅调整，手动merge的过程还是需要对框架有一定的了解。
在ch5中，`PROCESSOR`取代了`TASKMANAGER`的大部分功能，TaskManager只负责维护一个就绪队列，对于当前进程的操作由Processor承担，因此大部分的系统调用实现也转移到了Processor中。同时，由于维护进程的数据结构变为了双端队列，因此大部分关键信息需要被存储在TCB中，与当前进程保持同步。除了merge代码解决冲突之外，我对先前的代码并没有做较多修改，只是优化了一下`sys_get_time`的实现，降低了代码耦合度，并且把原先在Task Manger中维护的数据放到了更底层的TCB中保存。
### 实现`sys_spawn`系统调用
`spawn`的本质，其实就是将`fork`和`exec`结合在一起，参考`sys_fork`和`sys_exec`的实现即可。注意在实现的过程中要理解一些代码的用意，如`sys_fork`中：
```rust
pub fn sys_fork() -> isize {
    ···
    let trap_cx = new_task.inner_exclusive_access().get_trap_cx();
    trap_cx.x[10] = 0;
    add_task(new_task);
    new_pid as isize
}
```
这里在`add_task`之前特别获取了trap上下文，并且将a0，也就是返回值设置为0。这是因为子进程被创建并运行后会立即从trap处返回，此时我们就需要子进程的返回值为0来区分子进程与父进程。

还需要注意的是，`fork`和`exec`作为相对独立的两个过程，创建的进程存在两个阶段：第一阶段被`fork`创建，除了返回值之外，包括地址空间在内的一切都与父进程相同；第二阶段`exec`加载一个可执行程序到进程中，从elf文件中重新获取地址空间等部分信息，对进程进行修改，而对于我们的`spawn`来说，以上两个阶段需要合并为一个。因此我们可以直接从elf文件读取地址空间等信息进行TCB的创建，并且不用考虑特殊处理子进程的返回值。以下是我`spawn`的具体实现：
```rust
//os/src/syscall/process.rs

pub fn sys_spawn(path: *const u8) -> isize {
    trace!(
        "kernel:pid[{}]",
        current_task().unwrap().pid.0
    );
    let token = current_user_token();
    let path = translated_str(token, path);
    if let Some(data) = get_app_data_by_name(path.as_str()) {
        let task = current_task().unwrap();
        task.spawn(data) as isize
    } else {
        -1
    }
}

//os/src/task/task.rs

/// parent process fork the child process
    pub fn spawn(self: &Arc<Self>, elf_data: &[u8]) -> usize {
        let (memory_set, user_sp, entry_point) = MemorySet::from_elf(elf_data);
        let trap_cx_ppn = memory_set
            .translate(VirtAddr::from(TRAP_CONTEXT_BASE).into())
            .unwrap()
            .ppn();
        // ---- access parent PCB exclusively
        let mut parent_inner = self.inner_exclusive_access();
        // alloc a pid and a kernel stack in kernel space
        //创建一个新的TCB
        let pid_handle = pid_alloc();
        let kernel_stack = kstack_alloc();
        let kernel_stack_top = kernel_stack.get_top();
        let task_control_block = Arc::new(TaskControlBlock {
            pid: pid_handle,
            kernel_stack,
            inner: unsafe {
                UPSafeCell::new(TaskControlBlockInner {
                    trap_cx_ppn,
                    base_size: user_sp,
                    task_cx: TaskContext::goto_trap_return(kernel_stack_top),
                    task_status: TaskStatus::Ready,
                    memory_set,
                    parent: Some(Arc::downgrade(self)),
                    children: Vec::new(),
                    exit_code: 0,
                    heap_bottom: parent_inner.heap_bottom,
                    program_brk: parent_inner.program_brk,
                    syscall_times: [0; MAX_SYSCALL_NUM],
                    start_time: 0,
                    stride: 0,
                    priority: 16,
                })
            },
        });
        // add child
        parent_inner.children.push(task_control_block.clone());

        // modify kernel_sp in trap_cx
        // **** access child PCB exclusively
        let child_inner = task_control_block.inner_exclusive_access();
        let trap_cx = child_inner.get_trap_cx();
        *trap_cx = TrapContext::app_init_context(
            entry_point,
            user_sp,
            KERNEL_SPACE.exclusive_access().token(),
            self.kernel_stack.get_top(),
            trap_handler as usize,
        );
        add_task(task_control_block.clone()); //加入就绪队列
        task_control_block.pid.0  //获取新的pid
        // **** release child PCB
        // ---- release parent PCB
    }
```
### 实现stride算法
stride的思想可以简单归纳为运行总“路程”最短的进程优先调度，我们为每个进程维护一个路程和一个优先级，进程每次被调度会使运行路程加上一个步长，而步长大小由进程优先级决定。可以证明，如果预先设置一个较大的常数BigStride，则把步长设置为 $\frac{BigStride}{进程优先级}$ 可以使每个进程分配的时间将与其优先级成正比。

如题意，算法的实现思路已经非常清晰，即为每个进程额外维护一个路程`stride`和优先级`priority`，然后在调度时取最小`stride`即可。测评的性能要求不高，数据量也很小，因此我们直接for循环搜索即可。这里需要注意的是优先级下限为2，但是不设上限，最大可以达到`isize`的上限，因此常数`BigStride`不能太小，考虑到精度问题也尽量选取多个小数的最大公约数。具体实现过于简单，代码就不作赘述。